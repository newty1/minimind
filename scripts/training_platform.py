import os
import sys
import json
import subprocess
import threading
import time
import random
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict
import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
import torch
#TODO 
#1.æ·»åŠ å¤šå¡è®­ç»ƒæ”¯æŒ
#2.æ—¥å¿—bugä¿®å¤

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
__package__ = "scripts"
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

st.set_page_config(
    page_title="miniå¤§æ¨¡å‹è®­ç»ƒäº‘å¹³å°",
    page_icon="ğŸš€",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ ·å¼è®¾ç½®
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 900;
        text-align: center;
        margin-bottom: 1rem;
        color: #1f77b4;
    }
    .task-card {
        padding: 1rem;
        border-radius: 0.5rem;
        border: 1px solid #ddd;
        margin: 0.5rem 0;
    }
    .status-running {
        color: #28a745;
        font-weight: bold;
    }
    .status-completed {
        color: #007bff;
        font-weight: bold;
    }
    .status-failed {
        color: #dc3545;
        font-weight: bold;
    }
    .status-pending {
        color: #ffc107;
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# çº¿ç¨‹å®‰å…¨çš„æ•°æ®å­˜å‚¨ï¼ˆç”¨äºåå°çº¿ç¨‹ï¼‰
_thread_safe_data = {
    'task_logs': defaultdict(list),
    'task_metrics': defaultdict(dict),
}
_data_lock = threading.Lock()

# åˆå§‹åŒ–session state
if 'tasks' not in st.session_state:#ä»»åŠ¡
    st.session_state.tasks = {}
if 'task_processes' not in st.session_state:#ä»»åŠ¡è¿›ç¨‹
    st.session_state.task_processes = {}
if 'task_logs' not in st.session_state:#ä»»åŠ¡æ—¥å¿—
    st.session_state.task_logs = {}
if 'task_metrics' not in st.session_state:#ä»»åŠ¡æŒ‡æ ‡
    st.session_state.task_metrics = {}


def load_saved_tasks():
    """ä»æ–‡ä»¶åŠ è½½å·²ä¿å­˜çš„ä»»åŠ¡"""
    tasks_file = Path("../tasks.json")
    if tasks_file.exists():
        with open(tasks_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_tasks(tasks):
    """ä¿å­˜ä»»åŠ¡åˆ°æ–‡ä»¶"""
    tasks_file = Path("../tasks.json")
    with open(tasks_file, 'w', encoding='utf-8') as f:
        json.dump(tasks, f, indent=2, ensure_ascii=False, default=str)


def load_task_logs_from_file(task_id: str, max_lines: int = None):
    """ä»æ–‡ä»¶åŠ è½½ä»»åŠ¡æ—¥å¿—"""
    log_file = get_log_file_path(task_id)
    
    if not log_file.exists():
        return []
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # å»é™¤æ¢è¡Œç¬¦
        logs = [line.rstrip('\n\r') for line in lines]
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§è¡Œæ•°ï¼Œåªè¿”å›æœ€åNè¡Œ
        if max_lines and len(logs) > max_lines:
            return logs[-max_lines:]
        
        return logs
    except Exception as e:
        # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…å½±å“ç¨‹åºè¿è¡Œ
        return []


def extract_metrics_from_logs(logs: List[str]) -> Dict:
    """ä»æ—¥å¿—åˆ—è¡¨ä¸­æå–æ‰€æœ‰æŒ‡æ ‡ï¼ˆç”¨äºå›¾è¡¨ç»˜åˆ¶ï¼‰"""
    metrics = {
        'loss': [],
        'lr': [],
        'step': [],
        'epoch': [],
        'timestamp': []
    }
    
    for log_line in logs:
        parsed = parse_training_log(log_line)
        if parsed and 'loss' in parsed:
            metrics['loss'].append(parsed['loss'])
            metrics['lr'].append(parsed.get('lr', 0))
            metrics['step'].append(parsed.get('step', 0))
            metrics['epoch'].append(parsed.get('epoch', 0))
            # ä½¿ç”¨æ­¥æ•°ä½œä¸ºæ—¶é—´æˆ³ï¼ˆå› ä¸ºæ—¥å¿—æ–‡ä»¶ä¸­æ²¡æœ‰å®é™…æ—¶é—´æˆ³ï¼‰
            metrics['timestamp'].append(parsed.get('step', len(metrics['step'])))
    
    return metrics


def plot_training_metrics(metrics: Dict):
    """ç»˜åˆ¶è®­ç»ƒæŒ‡æ ‡å›¾è¡¨ï¼ˆLossã€Learning Rateã€Stepsã€Epochsï¼‰
    
    Args:
        metrics: åŒ…å« 'loss', 'lr', 'step', 'epoch' çš„å­—å…¸
    """
    if not metrics.get('loss'):
        st.info("æš‚æ— è®­ç»ƒæŒ‡æ ‡æ•°æ®ï¼Œè¯·ç­‰å¾…è®­ç»ƒå¼€å§‹...")
        return
    
    # åˆ›å»ºå›¾è¡¨å¸ƒå±€ï¼Œå¢åŠ å­å›¾é—´è·
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Loss æ›²çº¿', 'Learning Rate', 'Training Steps', 'Training Epochs'),
        vertical_spacing=0.18,  # å¢åŠ å‚ç›´é—´è·
        horizontal_spacing=0.15  # å¢åŠ æ°´å¹³é—´è·
    )
    
    # ä½¿ç”¨æ­¥æ•°ä½œä¸º X è½´ï¼ˆæ›´ç¬¦åˆè®­ç»ƒå¯è§†åŒ–ä¹ æƒ¯ï¼‰
    x_axis = metrics['step'] if metrics['step'] and len(metrics['step']) == len(metrics['loss']) else list(range(len(metrics['loss'])))
    
    # Loss æ›²çº¿ï¼ˆå·¦ä¸Šï¼‰- æ·»åŠ å¡«å……æ•ˆæœ
    fig.add_trace(
        go.Scatter(
            x=x_axis,
            y=metrics['loss'],
            mode='lines',
            name='Loss',
            line=dict(color='#1f77b4', width=2),
            fill='tozeroy',
            fillcolor='rgba(31, 119, 180, 0.1)',
            hovertemplate='<b>Loss</b><br>Step: %{x}<br>Loss: %{y:.4f}<extra></extra>'
        ),
        row=1, col=1
    )
    fig.update_xaxes(title_text="Step", row=1, col=1, showgrid=True, gridcolor='lightgray')
    fig.update_yaxes(title_text="Loss", row=1, col=1, showgrid=True, gridcolor='lightgray')
    
    # Learning Rateï¼ˆå³ä¸Šï¼‰
    if metrics['lr']:
        fig.add_trace(
            go.Scatter(
                x=x_axis,
                y=metrics['lr'],
                mode='lines',
                name='Learning Rate',
                line=dict(color='#ff7f0e', width=2),
                hovertemplate='<b>Learning Rate</b><br>Step: %{x}<br>LR: %{y:.2e}<extra></extra>'
            ),
            row=1, col=2
        )
        fig.update_xaxes(title_text="Step", row=1, col=2, showgrid=True, gridcolor='lightgray')
        fig.update_yaxes(title_text="Learning Rate", row=1, col=2, type="log", showgrid=True, gridcolor='lightgray')
    
    # Stepsï¼ˆå·¦ä¸‹ï¼‰- æ˜¾ç¤ºæ­¥æ•°è¿›åº¦
    if metrics['step']:
        fig.add_trace(
            go.Scatter(
                x=list(range(len(metrics['step']))),
                y=metrics['step'],
                mode='lines+markers',
                name='Steps',
                line=dict(color='#2ca02c', width=2),
                marker=dict(size=4, color='#2ca02c'),
                hovertemplate='<b>Steps</b><br>Index: %{x}<br>Step: %{y}<extra></extra>'
            ),
            row=2, col=1
        )
        fig.update_xaxes(title_text="Log Index", row=2, col=1, showgrid=True, gridcolor='lightgray')
        fig.update_yaxes(title_text="Step", row=2, col=1, showgrid=True, gridcolor='lightgray')
    
    # Epochsï¼ˆå³ä¸‹ï¼‰- æ˜¾ç¤ºè½®æ•°è¿›åº¦
    if metrics['epoch']:
        fig.add_trace(
            go.Scatter(
                x=list(range(len(metrics['epoch']))),
                y=metrics['epoch'],
                mode='lines+markers',
                name='Epochs',
                line=dict(color='#d62728', width=2),
                marker=dict(size=4, color='#d62728'),
                hovertemplate='<b>Epochs</b><br>Index: %{x}<br>Epoch: %{y}<extra></extra>'
            ),
            row=2, col=2
        )
        fig.update_xaxes(title_text="Log Index", row=2, col=2, showgrid=True, gridcolor='lightgray')
        fig.update_yaxes(title_text="Epoch", row=2, col=2, showgrid=True, gridcolor='lightgray')
    
    # æ›´æ–°æ•´ä½“å¸ƒå±€
    fig.update_layout(
        height=700,
        showlegend=False,
        title_text="ğŸ“Š è®­ç»ƒæŒ‡æ ‡ç›‘æ§",
        title_x=0.5,
        template="plotly_white",
        hovermode='x unified'
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("å½“å‰ Loss", f"{metrics['loss'][-1]:.4f}" if metrics['loss'] else "N/A")
    with col2:
        min_loss = min(metrics['loss']) if metrics['loss'] else 0
        st.metric("æœ€ä½ Loss", f"{min_loss:.4f}")
    with col3:
        st.metric("å½“å‰ LR", f"{metrics['lr'][-1]:.2e}" if metrics['lr'] and metrics['lr'][-1] > 0 else "N/A")
    with col4:
        st.metric("æ•°æ®ç‚¹æ•°", len(metrics['loss']))


def get_training_scripts():
    """è·å–å¯ç”¨çš„è®­ç»ƒè„šæœ¬"""
    trainer_dir = Path("../trainer")
    scripts = {}
    if trainer_dir.exists():
        for script in trainer_dir.glob("train_*.py"):#åŒ¹é…
            script_name = script.stem
            display_name = {
                "train_pretrain": "é¢„è®­ç»ƒ (Pretrain)",
                "train_full_sft": "ç›‘ç£å¾®è°ƒ (SFT)",
                "train_lora": "LoRAå¾®è°ƒ",
                "train_dpo": "DPOå¼ºåŒ–å­¦ä¹ ",
                "train_ppo": "PPOå¼ºåŒ–å­¦ä¹ ",
                "train_grpo": "GRPOå¼ºåŒ–å­¦ä¹ ",
                "train_spo": "SPOå¼ºåŒ–å­¦ä¹ ",
                "train_distill_reason": "æ¨ç†æ¨¡å‹è’¸é¦",
                "train_distillation": "æ¨¡å‹è’¸é¦",
            }.get(script_name, script_name)
            scripts[script_name] = {
                "display": display_name,
                "path": str(script)
            }
    return scripts


def get_datasets():
    """è·å–å¯ç”¨çš„æ•°æ®é›†"""
    dataset_dir = Path("../dataset")
    datasets = {}
    if dataset_dir.exists():
        for jsonl_file in dataset_dir.glob("*.jsonl"):
            datasets[jsonl_file.name] = str(jsonl_file)
    return datasets


def get_available_weights(save_dir="../out"):
    """è·å–å¯ç”¨çš„æ¨¡å‹æƒé‡å‰ç¼€åˆ—è¡¨"""
    weight_dir = Path(save_dir)
    weight_prefixes = set()
    
    if weight_dir.exists():
        # æ‰«ææ‰€æœ‰ .pth æ–‡ä»¶
        for pth_file in weight_dir.glob("*.pth"):
            filename = pth_file.stem  # å»æ‰ .pth æ‰©å±•å
            
            # åŒ¹é…æ ¼å¼ï¼š{prefix}_{hidden_size} æˆ– {prefix}_{hidden_size}_moe
            # ä¾‹å¦‚ï¼špretrain_512.pth -> pretrain
            #      full_sft_768_moe.pth -> full_sft
            match = re.match(r'^(.+?)_(\d+)(?:_moe)?$', filename)
            if match:
                prefix = match.group(1)
                weight_prefixes.add(prefix)
    
    # æ·»åŠ  "none" é€‰é¡¹ï¼ˆä»å¤´å¼€å§‹è®­ç»ƒï¼‰
    weight_prefixes.add("none")
    
    # æ’åºå¹¶è¿”å›åˆ—è¡¨
    sorted_prefixes = sorted(weight_prefixes)
    # å°† "none" æ”¾åœ¨æœ€å
    if "none" in sorted_prefixes:
        sorted_prefixes.remove("none")
        sorted_prefixes.append("none")
    
    return sorted_prefixes


def get_available_models():
    """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä»…Transformersæ ¼å¼ï¼‰"""
    models = {}
    
    # æ‰«æé¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰€æœ‰åŒ…å«config.jsonçš„ç›®å½•
    root_dir = Path("..")
    
    # é¢„å®šä¹‰çš„æ¨¡å‹ç›®å½•
    predefined_dirs = [
        Path("../MiniMind2"),
        Path("../MiniMind2-Small"),
        Path("../MiniMind2-MoE"),
        Path("../MiniMind2-R1"),
        Path("../MiniMind2-Small-R1"),
    ]
    
    # æ‰«æé¢„å®šä¹‰ç›®å½•
    predefined_paths = set()
    for model_dir in predefined_dirs:
        if model_dir.exists() and (model_dir / "config.json").exists():
            resolved_path = str(model_dir.resolve())
            predefined_paths.add(resolved_path)
            models[resolved_path] = {
                "name": model_dir.name,
                "path": resolved_path,
                "type": "transformers"
            }
    
    # åŠ¨æ€æ‰«æé¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰€æœ‰åŒ…å«config.jsonçš„ç›®å½•
    if root_dir.exists():
        for item in root_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                resolved_path = str(item.resolve())
                # è·³è¿‡å·²ç»æ·»åŠ çš„é¢„å®šä¹‰ç›®å½•
                if resolved_path in predefined_paths:
                    continue
                
                config_file = item / "config.json"
                if config_file.exists():
                    # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„Transformersæ¨¡å‹ç›®å½•ï¼ˆè‡³å°‘åŒ…å«config.jsonï¼‰
                    try:
                        models[resolved_path] = {
                            "name": item.name,
                            "path": resolved_path,
                            "type": "transformers"
                        }
                    except Exception:
                        # å¦‚æœæ£€æŸ¥è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œè·³è¿‡
                        continue
    
    return models


def load_model_for_inference(model_path, model_info, device="cuda:0"):
    """åŠ è½½æ¨¡å‹ç”¨äºæ¨ç†ï¼ˆä»…æ”¯æŒTransformersæ ¼å¼ï¼‰"""
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        # åªæ”¯æŒTransformersæ ¼å¼æ¨¡å‹
        if model_info["type"] != "transformers":
            st.warning("ä»…æ”¯æŒTransformersæ ¼å¼æ¨¡å‹ï¼Œè¯·å…ˆåœ¨æ¨¡å‹ç®¡ç†é¡µé¢å°†PyTorchæ¨¡å‹è½¬æ¢ä¸ºTransformersæ ¼å¼")
            return None, None
        
        # åŠ è½½Transformersæ ¼å¼æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16 if device.startswith("cuda") else torch.float32
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        model = model.eval().to(device)
        return model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        st.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return None, None


def release_model_from_gpu():
    """é‡Šæ”¾æ¨¡å‹å ç”¨çš„GPUå†…å­˜"""
    if 'current_model' in st.session_state and st.session_state.current_model is not None:
        try:
            # å°†æ¨¡å‹ç§»åˆ°CPU
            st.session_state.current_model = st.session_state.current_model.cpu()
            # åˆ é™¤æ¨¡å‹å¼•ç”¨
            del st.session_state.current_model
            # æ¸…ç©ºCUDAç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            # æ¸…ç†session state
            st.session_state.current_model = None
            st.session_state.current_tokenizer = None
            st.session_state.current_model_path = None
            return True
        except Exception as e:
            st.error(f"é‡Šæ”¾GPUå¤±è´¥: {str(e)}")
            return False
    return True


def process_assistant_content(content):
    """å¤„ç†åŠ©æ‰‹å›å¤å†…å®¹ï¼ˆå¤„ç†æ¨ç†æ ‡ç­¾ç­‰ï¼‰"""
    if '<think>' in content and '</think>' in content:
        content = re.sub(
            r'(<think>)(.*?)(</think>)',
            r'<details style="font-style: italic; background: rgba(222, 222, 222, 0.5); padding: 10px; border-radius: 10px;"><summary style="font-weight:bold;">æ¨ç†å†…å®¹ï¼ˆå±•å¼€ï¼‰</summary>\2</details>',
            content,
            flags=re.DOTALL
        )
    return content


def setup_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def parse_model_config_from_filename(filename: str):
    """ä»æ–‡ä»¶åè§£ææ¨¡å‹é…ç½®"""
    import re
    filename_stem = Path(filename).stem
    
    # è§£æhidden_size
    hidden_size_match = re.search(r'_(\d+)(?:_moe)?(?:\.pth)?$', filename_stem)
    hidden_size = int(hidden_size_match.group(1)) if hidden_size_match else 512
    
    # è§£ææ˜¯å¦ä½¿ç”¨MoE
    use_moe = '_moe' in filename_stem
    
    # æ ¹æ®hidden_sizeæ¨æ–­num_hidden_layers
    # 512 -> 8å±‚, 768 -> 16å±‚, 640 -> 8å±‚(MoE)
    if hidden_size == 512:
        num_hidden_layers = 8
    elif hidden_size == 768:
        num_hidden_layers = 16
    elif hidden_size == 640:
        num_hidden_layers = 8  # MoEé€šå¸¸8å±‚
    else:
        # é»˜è®¤æ ¹æ®hidden_sizeä¼°ç®—
        num_hidden_layers = 8 if hidden_size <= 512 else 16
    
    return {
        'hidden_size': hidden_size,
        'num_hidden_layers': num_hidden_layers,
        'use_moe': use_moe
    }


def convert_torch_to_transformers(
    torch_path: str, 
    output_path: str, 
    config: Dict,
    convert_type: str = "llama",  # "llama" æˆ– "minimind"
    dtype: str = "float16"
):
    """è½¬æ¢PyTorchæ¨¡å‹åˆ°Transformersæ ¼å¼"""
    try:
        from transformers import AutoTokenizer, LlamaConfig, LlamaForCausalLM
        from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        torch_dtype = torch.float16 if dtype == "float16" else torch.bfloat16
        
        # åŠ è½½PyTorchæƒé‡
        state_dict = torch.load(torch_path, map_location=device)
        
        # åˆ›å»ºé…ç½®ï¼ˆä½¿ç”¨é»˜è®¤çš„max_position_embeddings=32768ï¼‰
        lm_config = MiniMindConfig(
            hidden_size=config['hidden_size'],
            num_hidden_layers=config['num_hidden_layers'],
            use_moe=config['use_moe']
        )
        
        model_params = 0
        
        if convert_type == "minimind":
            # è½¬æ¢ä¸ºMiniMindæ ¼å¼
            MiniMindConfig.register_for_auto_class()
            MiniMindForCausalLM.register_for_auto_class("AutoModelForCausalLM")
            model = MiniMindForCausalLM(lm_config)
            model.load_state_dict(state_dict, strict=False)
            model = model.to(torch_dtype)
            model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            model.save_pretrained(output_path, safe_serialization=False)
            
        else:  # convert_type == "llama"
            # è½¬æ¢ä¸ºLlamaå…¼å®¹æ ¼å¼
            llama_config = LlamaConfig(
                vocab_size=lm_config.vocab_size,
                hidden_size=lm_config.hidden_size,
                intermediate_size=64 * ((int(lm_config.hidden_size * 8 / 3) + 64 - 1) // 64),
                num_hidden_layers=lm_config.num_hidden_layers,
                num_attention_heads=lm_config.num_attention_heads,
                num_key_value_heads=lm_config.num_key_value_heads,
                max_position_embeddings=lm_config.max_position_embeddings,
                rms_norm_eps=lm_config.rms_norm_eps,
                rope_theta=lm_config.rope_theta,
                tie_word_embeddings=True
            )
            llama_model = LlamaForCausalLM(llama_config)
            llama_model.load_state_dict(state_dict, strict=False)
            llama_model = llama_model.to(torch_dtype)
            model_params = sum(p.numel() for p in llama_model.parameters() if p.requires_grad)
            llama_model.save_pretrained(output_path)
        
        # ä¿å­˜tokenizerï¼ˆä»æ¨¡å‹ç›®å½•åŠ è½½ï¼‰
        tokenizer = AutoTokenizer.from_pretrained('../model/')
        tokenizer.save_pretrained(output_path)
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True, f"âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºTransformers-{convert_type.upper()}æ ¼å¼\nğŸ“Š å‚æ•°é‡: {model_params / 1e6:.2f}M ({model_params / 1e9:.3f}B)\nğŸ“ ä¿å­˜è·¯å¾„: {output_path}"
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ è½¬æ¢å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return False, error_msg


def convert_transformers_to_torch(
    transformers_path: str,
    output_path: str
):
    """è½¬æ¢Transformersæ¨¡å‹åˆ°PyTorchæ ¼å¼"""
    try:
        from transformers import AutoModelForCausalLM
        
        # åŠ è½½Transformersæ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            transformers_path,
            trust_remote_code=True,
            torch_dtype=torch.float32  # ä¿å­˜ä¸ºfloat32ä»¥ç¡®ä¿å…¼å®¹æ€§
        )
        
        # ä¿å­˜ä¸ºPyTorchæ ¼å¼
        torch.save(model.state_dict(), output_path)
        
        # è®¡ç®—å‚æ•°é‡
        model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # æ¸…ç†GPUç¼“å­˜
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True, f"âœ… æ¨¡å‹å·²æˆåŠŸè½¬æ¢ä¸ºPyTorchæ ¼å¼\nğŸ“Š å‚æ•°é‡: {model_params / 1e6:.2f}M ({model_params / 1e9:.3f}B)\nğŸ“ ä¿å­˜è·¯å¾„: {output_path}"
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ è½¬æ¢å¤±è´¥: {str(e)}\n{traceback.format_exc()}"
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return False, error_msg


def generate_task_id():
    """ç”Ÿæˆä»»åŠ¡ID"""
    return f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(st.session_state.tasks)}"


def parse_training_log(log_line: str) -> Optional[Dict]:
    """è§£æè®­ç»ƒæ—¥å¿—ï¼Œæå–æŒ‡æ ‡"""
    import re
    metrics = {}
    
    # åŒ¹é…Loss
    loss_match = re.search(r'loss[:\s]+([\d.]+)', log_line, re.I)
    if loss_match:
        metrics['loss'] = float(loss_match.group(1))
    
    # åŒ¹é…å­¦ä¹ ç‡
    lr_match = re.search(r'lr[:\s]+([\d.e-]+)', log_line, re.I)
    if lr_match:
        metrics['lr'] = float(lr_match.group(1))
    
    # åŒ¹é…æ­¥æ•°
    step_match = re.search(r'\((\d+)/(\d+)\)', log_line)
    if step_match:
        metrics['step'] = int(step_match.group(1))
        metrics['total_steps'] = int(step_match.group(2))
    
    # åŒ¹é…Epoch
    epoch_match = re.search(r'Epoch\[(\d+)/(\d+)\]', log_line)
    if epoch_match:
        metrics['epoch'] = int(epoch_match.group(1))
        metrics['total_epochs'] = int(epoch_match.group(2))
    
    return metrics if metrics else None


def get_log_file_path(task_id: str):
    """è·å–ä»»åŠ¡æ—¥å¿—æ–‡ä»¶è·¯å¾„"""
    # ä½¿ç”¨ä¸ä»£ç ç¬¬25è¡Œç›¸åŒçš„è·¯å¾„è·å–æ–¹å¼ï¼Œç¡®ä¿ä¸€è‡´æ€§
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œé¿å…å·¥ä½œç›®å½•å˜åŒ–å¯¼è‡´çš„é—®é¢˜
    script_dir = os.path.dirname(os.path.abspath(__file__))  # scripts ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    project_root = os.path.abspath(os.path.join(script_dir, '..'))  # é¡¹ç›®æ ¹ç›®å½•ï¼ˆç»å¯¹è·¯å¾„ï¼‰
    logs_dir = Path(project_root) / "logs"
    logs_dir.mkdir(parents=True, exist_ok=True)
    return logs_dir / f"{task_id}.log"


def process_log_line(line: str, task_id: str, task_logs: list, task_metrics: dict):
    """å¤„ç†å•è¡Œæ—¥å¿—ï¼šæ·»åŠ æ—¥å¿—ã€è§£ææŒ‡æ ‡ã€æ›´æ–°çº¿ç¨‹å®‰å…¨å­˜å‚¨"""
    line = line.strip()
    if not line:  # å¿½ç•¥ç©ºè¡Œ
        return
    
    # 1. æ·»åŠ æ—¥å¿—åˆ°æœ¬åœ°åˆ—è¡¨
    task_logs.append(line)
    
    # 2. å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
    try:
        log_file = get_log_file_path(task_id)
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(line + '\n')
    except Exception as e:
        # æ–‡ä»¶å†™å…¥å¤±è´¥ä¸å½±å“ç¨‹åºè¿è¡Œ
        pass
    
    # 3. ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼æ›´æ–°æ—¥å¿—ï¼ˆåªä¿ç•™æœ€è¿‘100è¡Œï¼‰
    with _data_lock:
        _thread_safe_data['task_logs'][task_id] = task_logs[-100:]
    
    # 4. è§£ææŒ‡æ ‡
    metrics = parse_training_log(line)
    if not metrics:
        return
    
    # 5. æ·»åŠ æŒ‡æ ‡åˆ°æœ¬åœ°å­—å…¸
    timestamp = time.time()
    if 'loss' in metrics:
        task_metrics['loss'].append(metrics['loss'])
        task_metrics['lr'].append(metrics.get('lr', 0))
        task_metrics['step'].append(metrics.get('step', 0))
        task_metrics['epoch'].append(metrics.get('epoch', 0))
        task_metrics['timestamp'].append(timestamp)
        
        # 6. æ›´æ–°ä»»åŠ¡æŒ‡æ ‡ï¼ˆåªä¿ç•™æœ€è¿‘500ä¸ªç‚¹ï¼‰
        for key in task_metrics:
            if len(task_metrics[key]) > 500:
                task_metrics[key] = task_metrics[key][-500:]
        
        # 7. ä½¿ç”¨çº¿ç¨‹å®‰å…¨çš„æ–¹å¼æ›´æ–°æŒ‡æ ‡
        with _data_lock:
            _thread_safe_data['task_metrics'][task_id] = {
                k: v.copy() if isinstance(v, list) else v 
                for k, v in task_metrics.items()
            }


def monitor_training_task(task_id: str, process: subprocess.Popen, task_config: Dict):
    """ç›‘æ§è®­ç»ƒä»»åŠ¡è¿›ç¨‹ï¼ˆåœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼‰"""
    task_logs = []
    task_metrics = {
        'loss': [],
        'lr': [],
        'step': [],
        'epoch': [],
        'timestamp': []
    }
    
    if process.stdout:
        try:
            while True:
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                if process.poll() is not None:
                    # è¿›ç¨‹å·²ç»“æŸï¼Œè¯»å–å‰©ä½™è¾“å‡º
                    remaining = process.stdout.read()
                    if remaining:
                        for line in remaining.splitlines():
                            process_log_line(line, task_id, task_logs, task_metrics)
                    break
                
                # å°è¯•è¯»å–ä¸€è¡Œï¼ˆéé˜»å¡ï¼‰
                line = process.stdout.readline()
                if line:
                    process_log_line(line, task_id, task_logs, task_metrics)
                else:
                    # æ²¡æœ‰æ–°è¾“å‡ºï¼ŒçŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
                    time.sleep(0.1)
        except Exception as e:
            # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­
            with _data_lock:
                _thread_safe_data['task_logs'][task_id] = task_logs[-100:] + [f"[é”™è¯¯] æ—¥å¿—è¯»å–å¼‚å¸¸: {str(e)}"]
    
    # ç­‰å¾…è¿›ç¨‹ç»“æŸ
    process.wait()
    
    # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆéœ€è¦åŒæ­¥åˆ°ä¸»çº¿ç¨‹ï¼Œè¿™é‡Œå…ˆä¿å­˜åˆ°æ–‡ä»¶ï¼‰
    # æ³¨æ„ï¼šä¸èƒ½åœ¨è¿™é‡Œç›´æ¥ä¿®æ”¹ st.session_stateï¼Œéœ€è¦åœ¨ä¸»çº¿ç¨‹ä¸­å¤„ç†
    final_status = {
        'status': 'completed' if process.returncode == 0 else 'failed',
        'end_time': datetime.now().isoformat(),
        'returncode': process.returncode
    }
    
    # ä¿å­˜æœ€ç»ˆçŠ¶æ€åˆ°çº¿ç¨‹å®‰å…¨å­˜å‚¨
    with _data_lock:
        _thread_safe_data['task_final_status'] = _thread_safe_data.get('task_final_status', {})
        _thread_safe_data['task_final_status'][task_id] = final_status
        if process.returncode != 0:
            _thread_safe_data['task_final_status'][task_id]['error'] = "è®­ç»ƒè¿›ç¨‹å¼‚å¸¸é€€å‡º"


def start_training_task(task_config: Dict) -> str:
    """å¯åŠ¨è®­ç»ƒä»»åŠ¡"""
    task_id = generate_task_id()
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    script_path = task_config['training_script']
    cmd = ['python', script_path]
    
    # æ·»åŠ å‚æ•°
    param_mapping = {
        'epochs': '--epochs',
        'batch_size': '--batch_size',
        'learning_rate': '--learning_rate',
        'hidden_size': '--hidden_size',
        'num_hidden_layers': '--num_hidden_layers',
        'max_seq_len': '--max_seq_len',
        'use_moe': '--use_moe',
        'data_path': '--data_path',
        'from_weight': '--from_weight',
        'from_resume': '--from_resume',
        'save_dir': '--save_dir',
        'save_weight': '--save_weight',
        'device': '--device',
        'dtype': '--dtype',
    }
    
    for key, arg_name in param_mapping.items():
        if key in task_config and task_config[key] is not None:
            if key == 'use_moe':#åç»­æ‹“å±•
                cmd.extend([arg_name, str(task_config[key])])
            elif key == 'from_resume':
                cmd.extend([arg_name, str(task_config[key])])
            elif key == 'data_path':
                cmd.extend([arg_name, str(task_config[key])])
            else:
                cmd.extend([arg_name, str(task_config[key])])
    
    # å¤„ç† use_wandb å‚æ•°ï¼ˆaction="store_true" ç±»å‹ï¼Œåªéœ€è¦æ·»åŠ å‚æ•°åï¼Œä¸éœ€è¦å€¼ï¼‰
    if task_config.get('use_wandb', False):
        cmd.append('--use_wandb')
    
    # å¯åŠ¨è®­ç»ƒè¿›ç¨‹
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶Pythonè¾“å‡ºä¸ç¼“å†²ï¼ˆç¡®ä¿å®æ—¶è¾“å‡ºï¼‰
        env = os.environ.copy()
        env['PYTHONUNBUFFERED'] = '1'
        
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            cwd=str(Path("../trainer").resolve()),
            bufsize=1,  # è¡Œç¼“å†²
            env=env  # ä½¿ç”¨ä¿®æ”¹åçš„ç¯å¢ƒå˜é‡
        )
        
        # ä¿å­˜ä»»åŠ¡ä¿¡æ¯
        st.session_state.tasks[task_id] = {
            'id': task_id,
            'config': task_config,
            'status': 'running',
            'start_time': datetime.now().isoformat(),
            'pid': process.pid
        }
        st.session_state.task_processes[task_id] = process
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(
            target=monitor_training_task,
            args=(task_id, process, task_config),
            daemon=True
        )
        monitor_thread.start()
        
        save_tasks(st.session_state.tasks)
        return task_id
        
    except Exception as e:
        st.error(f"å¯åŠ¨è®­ç»ƒä»»åŠ¡å¤±è´¥: {str(e)}")
        return None


def stop_training_task(task_id: str):
    """åœæ­¢è®­ç»ƒä»»åŠ¡"""
    if task_id in st.session_state.task_processes:
        process = st.session_state.task_processes[task_id]
        try:
            process.terminate()
            time.sleep(2)
            if process.poll() is None:
                process.kill()
            
            if task_id in st.session_state.tasks:
                st.session_state.tasks[task_id]['status'] = 'stopped'
                st.session_state.tasks[task_id]['end_time'] = datetime.now().isoformat()
                save_tasks(st.session_state.tasks)
            
            del st.session_state.task_processes[task_id]
            return True
        except Exception as e:
            st.error(f"åœæ­¢ä»»åŠ¡å¤±è´¥: {str(e)}")
            return False
    return False


def sync_thread_data_to_session():
    """ä»çº¿ç¨‹å®‰å…¨å­˜å‚¨åŒæ­¥æ•°æ®åˆ°session_state"""
    with _data_lock:
        # åŒæ­¥æ—¥å¿—ï¼ˆä¼˜å…ˆä»æ–‡ä»¶è¯»å–å®Œæ•´æ—¥å¿—ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…å­˜ä¸­çš„ï¼‰
        for task_id, logs in _thread_safe_data['task_logs'].items():
            # å°è¯•ä»æ–‡ä»¶è¯»å–å®Œæ•´æ—¥å¿—
            file_logs = load_task_logs_from_file(task_id)
            if file_logs:
                # å¦‚æœæ–‡ä»¶ä¸­æœ‰æ—¥å¿—ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­çš„ï¼ˆæ›´å®Œæ•´ï¼‰
                st.session_state.task_logs[task_id] = file_logs
            else:
                # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œä½¿ç”¨å†…å­˜ä¸­çš„ï¼ˆæœ€è¿‘100è¡Œï¼‰
                st.session_state.task_logs[task_id] = logs.copy()
        
        # åŒæ­¥æŒ‡æ ‡
        for task_id, metrics in _thread_safe_data['task_metrics'].items():
            st.session_state.task_metrics[task_id] = {
                k: v.copy() if isinstance(v, list) else v 
                for k, v in metrics.items()
            }
        
        # åŒæ­¥æœ€ç»ˆçŠ¶æ€
        if 'task_final_status' in _thread_safe_data:
            for task_id, final_status in _thread_safe_data['task_final_status'].items():
                if task_id in st.session_state.tasks:
                    st.session_state.tasks[task_id].update(final_status)
                    if final_status['status'] in ['completed', 'failed']:
                        save_tasks(st.session_state.tasks)
            # æ¸…ç†å·²å¤„ç†çš„çŠ¶æ€
            _thread_safe_data['task_final_status'] = {}


def main():
    
    # é¡µé¢æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸš€ Miniå¤§æ¨¡å‹è®­ç»ƒäº‘å¹³å°</h1>', unsafe_allow_html=True)
    
    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("å¯¼èˆª")
        page = st.radio(
            "é€‰æ‹©é¡µé¢",
            ["åˆ›å»ºè®­ç»ƒä»»åŠ¡", "ä»»åŠ¡ç›‘æ§", "ä»»åŠ¡ç®¡ç†", "æ¨¡å‹ç®¡ç†", "æ¨¡å‹ä½¿ç”¨"],
            index=0
        )
        
        # å¦‚æœåˆ‡æ¢åˆ°å…¶ä»–é¡µé¢ï¼Œè‡ªåŠ¨é‡Šæ”¾GPUï¼ˆå¦‚æœæ¨¡å‹å·²åŠ è½½ï¼‰
        if page != "æ¨¡å‹ä½¿ç”¨" and 'current_model' in st.session_state and st.session_state.current_model is not None:
            if st.session_state.get('auto_release_gpu', True):
                try:
                    release_model_from_gpu()
                except:
                    pass  # é™é»˜å¤„ç†é”™è¯¯ï¼Œé¿å…å½±å“é¡µé¢åˆ‡æ¢
        
        st.markdown("---")
        st.header("ç³»ç»Ÿä¿¡æ¯")
        st.info("Miniå¤§æ¨¡å‹è®­ç»ƒäº‘å¹³å°\n\nç®€åŒ–å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒæµç¨‹")
    
    # åŠ è½½å·²ä¿å­˜çš„ä»»åŠ¡
    if not st.session_state.tasks:
        saved_tasks = load_saved_tasks()
        st.session_state.tasks.update(saved_tasks)
    
    # åŒæ­¥çº¿ç¨‹æ•°æ®åˆ°session_stateï¼ˆæ¯æ¬¡é¡µé¢åˆ·æ–°æ—¶ï¼‰
    sync_thread_data_to_session()
    
    # åˆ›å»ºè®­ç»ƒä»»åŠ¡é¡µé¢
    if page == "åˆ›å»ºè®­ç»ƒä»»åŠ¡":
        st.header("ğŸ“ åˆ›å»ºè®­ç»ƒä»»åŠ¡")
        
        # å°†æ–‡ä»¶ä¸Šä¼ ç§»åˆ°è¡¨å•å¤–éƒ¨ï¼Œä½¿å…¶å¯ä»¥ç«‹å³å¤„ç†
        st.subheader("æ•°æ®é›†é…ç½®")
        
        # æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½
        uploaded_file = st.file_uploader(
            "ğŸ“¤ ä¸Šä¼ è®­ç»ƒæ•°æ®é›†æ–‡ä»¶",
            type=['jsonl'],
            help="æ”¯æŒ .jsonl æ ¼å¼çš„æ•°æ®é›†æ–‡ä»¶ï¼Œä¸Šä¼ åå°†ä¿å­˜åˆ° dataset ç›®å½•"
        )
        
        if uploaded_file is not None:
            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ° dataset ç›®å½•
            dataset_dir = Path("../dataset")
            dataset_dir.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            file_path = dataset_dir / uploaded_file.name
            if file_path.exists():
                st.warning(f"âš ï¸ æ–‡ä»¶ `{uploaded_file.name}` å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–ã€‚")
            
            # ä¿å­˜æ–‡ä»¶
            try:
                with open(file_path, 'wb') as f:
                    f.write(uploaded_file.getbuffer())
                
                file_size_mb = file_path.stat().st_size / (1024 * 1024)
                st.success(f"âœ… æ–‡ä»¶å·²æˆåŠŸä¸Šä¼ åˆ° `{file_path}` (å¤§å°: {file_size_mb:.2f} MB)")
                # è‡ªåŠ¨åˆ·æ–°é¡µé¢ï¼Œä½¿æ–°æ–‡ä»¶ç«‹å³æ˜¾ç¤ºåœ¨ä¸‹æ‹‰åˆ—è¡¨ä¸­
                st.rerun()
            except Exception as e:
                st.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        
        with st.form("training_task_form"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("åŸºç¡€é…ç½®")
                
                # è®­ç»ƒç±»å‹
                training_scripts = get_training_scripts()
                if not training_scripts:
                    st.error("æœªæ‰¾åˆ°è®­ç»ƒè„šæœ¬ï¼Œè¯·ç¡®ä¿trainerç›®å½•å­˜åœ¨")
                    st.stop()
                
                training_type = st.selectbox(
                    "è®­ç»ƒç±»å‹",
                    options=list(training_scripts.keys()),
                    format_func=lambda x: training_scripts[x]['display']
                )
                training_script = training_scripts[training_type]['path']
                
                # æ•°æ®é›†é€‰æ‹©ï¼ˆé‡æ–°è·å–ï¼Œç¡®ä¿åŒ…å«æ–°ä¸Šä¼ çš„æ–‡ä»¶ï¼‰
                st.subheader("é€‰æ‹©æ•°æ®é›†")
                datasets = get_datasets()
                
                if not datasets:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ï¼")
                    st.info("ğŸ“ è¯·ä¸Šä¼ æ•°æ®é›†æ–‡ä»¶ï¼ˆ.jsonlæ ¼å¼ï¼‰æˆ–å°†å…¶æ”¾ç½®åœ¨ `dataset` ç›®å½•ä¸‹ã€‚")
                    data_path = None  # å¦‚æœæ²¡æœ‰æ•°æ®é›†ï¼Œè®¾ç½®ä¸º None
                else:
                    selected_dataset = st.selectbox("é€‰æ‹©æ•°æ®é›†", options=list(datasets.keys()))
                    data_path = datasets[selected_dataset]
                
                # æ¨¡å‹é…ç½®
                st.subheader("æ¨¡å‹é…ç½®")
                hidden_size = st.number_input("éšè—å±‚ç»´åº¦ (hidden_size)", min_value=256, max_value=2048, value=512, step=64)
                num_hidden_layers = st.number_input("éšè—å±‚æ•°é‡ (num_hidden_layers)", min_value=4, max_value=32, value=8, step=2)
                max_seq_len = st.number_input("æœ€å¤§åºåˆ—é•¿åº¦ (max_seq_len)", min_value=128, max_value=8192, value=340, step=64)
                use_moe = st.checkbox("ä½¿ç”¨MoEæ¶æ„", value=False)
                
            with col2:
                st.subheader("è®­ç»ƒé…ç½®")
                
                epochs = st.number_input("è®­ç»ƒè½®æ•° (epochs)", min_value=1, max_value=100, value=2)
                batch_size = st.number_input("æ‰¹æ¬¡å¤§å° (batch_size)", min_value=1, max_value=128, value=16, step=4)
                learning_rate = st.number_input("å­¦ä¹ ç‡ (learning_rate)", min_value=1e-8, max_value=1e-3, value=5e-7, format="%.2e", step=1e-7)
                
                accumulation_steps = st.number_input("æ¢¯åº¦ç´¯ç§¯æ­¥æ•°", min_value=1, max_value=32, value=1)
                grad_clip = st.number_input("æ¢¯åº¦è£å‰ªé˜ˆå€¼", min_value=0.1, max_value=10.0, value=1.0, step=0.1)
                
                st.subheader("å…¶ä»–é…ç½®")
                device = st.selectbox("è®­ç»ƒè®¾å¤‡", options=["cuda:0", "cuda:1", "cpu"], index=0)
                dtype = st.selectbox("æ•°æ®ç±»å‹", options=["bfloat16", "float16", "float32"], index=0)
                
                # è·å–å¯ç”¨çš„æƒé‡å‰ç¼€
                available_weights = get_available_weights(save_dir="../out")
                if not available_weights or available_weights == ["none"]:
                    st.warning("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æƒé‡æ–‡ä»¶ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒ")
                    from_weight = "none"
                else:
                    # è®¾ç½®é»˜è®¤å€¼ï¼šå¦‚æœæœ‰ "pretrain" åˆ™ä¼˜å…ˆé€‰æ‹©ï¼Œå¦åˆ™é€‰æ‹©ç¬¬ä¸€ä¸ªé "none" çš„
                    default_index = 0
                    if "pretrain" in available_weights:
                        default_index = available_weights.index("pretrain")
                    elif "none" in available_weights:
                        # å¦‚æœåªæœ‰ "none"ï¼Œé€‰æ‹©å®ƒ
                        default_index = available_weights.index("none")
                    
                    from_weight = st.selectbox(
                        "åŸºç¡€æƒé‡ (from_weight)",
                        options=available_weights,
                        index=default_index,
                        help="é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹æƒé‡å‰ç¼€ã€‚é€‰æ‹© 'none' è¡¨ç¤ºä»å¤´å¼€å§‹è®­ç»ƒã€‚"
                    )
                
                from_resume = st.checkbox("å¯ç”¨æ–­ç‚¹ç»­è®­ (from_resume)", value=False)
                use_wandb = st.checkbox("å¯ç”¨WandB/SwanLabè®°å½• (use_wandb)", value=False, help="å¯ç”¨åå°†ä½¿ç”¨WandBæˆ–SwanLabè®°å½•è®­ç»ƒè¿‡ç¨‹")
                
                save_dir = st.text_input("ä¿å­˜ç›®å½•", value="../out")
                save_weight = st.text_input("æƒé‡åç§°å‰ç¼€", value="full_sft")
            
            # æäº¤æŒ‰é’®
            submitted = st.form_submit_button("ğŸš€ æäº¤è®­ç»ƒä»»åŠ¡", use_container_width=True)
            
            if submitted:
                # éªŒè¯æ•°æ®é›†è·¯å¾„
                if 'data_path' not in locals() or data_path is None:
                    st.error("âŒ è¯·å…ˆä¸Šä¼ æˆ–é€‰æ‹©æ•°æ®é›†æ–‡ä»¶ï¼")
                    st.stop()
                
                task_config = {
                    'training_script': training_script,
                    'training_type': training_type,
                    'epochs': epochs,
                    'batch_size': batch_size,
                    'learning_rate': learning_rate,
                    'hidden_size': hidden_size,
                    'num_hidden_layers': num_hidden_layers,
                    'max_seq_len': max_seq_len,
                    'use_moe': 1 if use_moe else 0,
                    'data_path': data_path,
                    'from_weight': from_weight,
                    'from_resume': 1 if from_resume else 0,
                    'use_wandb': use_wandb,
                    'save_dir': save_dir,
                    'save_weight': save_weight,
                    'device': device,
                    'dtype': dtype,
                    'accumulation_steps': accumulation_steps,
                    'grad_clip': grad_clip,
                }
                
                task_id = start_training_task(task_config)
                if task_id:
                    st.success(f"âœ… è®­ç»ƒä»»åŠ¡å·²æäº¤ï¼ä»»åŠ¡ID: {task_id}")
                    st.balloons()
                else:
                    st.error("âŒ æäº¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
    
    # ä»»åŠ¡ç›‘æ§é¡µé¢
    elif page == "ä»»åŠ¡ç›‘æ§":
        st.header("ğŸ“Š ä»»åŠ¡ç›‘æ§")
        
        if not st.session_state.tasks:
            st.info("æš‚æ— è®­ç»ƒä»»åŠ¡")
        else:
            # é€‰æ‹©è¦ç›‘æ§çš„ä»»åŠ¡
            running_tasks = {k: v for k, v in st.session_state.tasks.items() if v['status'] == 'running'}
            
            if not running_tasks:
                st.info("å½“å‰æ²¡æœ‰è¿è¡Œä¸­çš„ä»»åŠ¡")
            else:
                selected_task_id = st.selectbox(
                    "é€‰æ‹©ä»»åŠ¡",
                    options=list(running_tasks.keys()),
                    format_func=lambda x: f"{x} - {running_tasks[x]['config'].get('training_type', 'N/A')}"
                )
                
                if selected_task_id:
                    task = st.session_state.tasks[selected_task_id]
                    
                    # ä»»åŠ¡ä¿¡æ¯å¡ç‰‡
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("ä»»åŠ¡çŠ¶æ€", task['status'])
                    with col2:
                        start_time = datetime.fromisoformat(task['start_time'])
                        elapsed = datetime.now() - start_time
                        st.metric("è¿è¡Œæ—¶é—´", f"{elapsed.seconds // 60}åˆ†é’Ÿ")
                    with col3:
                        if 'pid' in task:
                            st.metric("è¿›ç¨‹ID", task['pid'])
                    with col4:
                        if 'config' in task:
                            st.metric("è®­ç»ƒç±»å‹", task['config'].get('training_type', 'N/A'))
                    
                    # è®­ç»ƒæŒ‡æ ‡å›¾è¡¨
                    # ä¼˜å…ˆä»æ—¥å¿—æ–‡ä»¶æå–æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…å­˜ä¸­çš„
                    file_logs = load_task_logs_from_file(selected_task_id)
                    if file_logs:
                        metrics = extract_metrics_from_logs(file_logs)
                    elif selected_task_id in st.session_state.task_metrics:
                        metrics = st.session_state.task_metrics[selected_task_id]
                    else:
                        metrics = {'loss': [], 'lr': [], 'step': [], 'epoch': [], 'timestamp': []}
                    
                    plot_training_metrics(metrics)
                    
                    # å®æ—¶æ—¥å¿—
                    col_log1, col_log2 = st.columns([3, 1])
                    with col_log1:
                        st.subheader("ğŸ“‹ è®­ç»ƒæ—¥å¿—")
                    with col_log2:
                        if st.button("ğŸ”„ åˆ·æ–°æ—¥å¿—", key=f"refresh_logs_{selected_task_id}", use_container_width=True):
                            # å¼ºåˆ¶åŒæ­¥æ—¥å¿—
                            sync_thread_data_to_session()
                            st.rerun()
                    
                    # ä¼˜å…ˆä»æ–‡ä»¶è¯»å–æ—¥å¿—ï¼ˆå®Œæ•´æ—¥å¿—ï¼‰
                    file_logs = load_task_logs_from_file(selected_task_id)
                    if file_logs:
                        logs = file_logs
                    elif selected_task_id in st.session_state.task_logs:
                        logs = st.session_state.task_logs[selected_task_id]
                    else:
                        logs = []
                    
                    if logs:
                        # æ˜¾ç¤ºæœ€è¿‘500è¡Œæ—¥å¿—ï¼ˆä½¿ç”¨code blockä»¥ä¾¿æ›´å¥½åœ°æ˜¾ç¤ºï¼‰
                        display_logs = logs[-500:] if len(logs) > 500 else logs
                        log_text = "\n".join(display_logs)
                        st.code(log_text, language=None)
                        st.caption(f"æ˜¾ç¤ºæœ€è¿‘ {len(display_logs)} è¡Œæ—¥å¿—ï¼ˆå…± {len(logs)} è¡Œï¼‰")
                        if len(logs) > 500:
                            st.info(f"ğŸ’¡ æ—¥å¿—æ–‡ä»¶åŒ…å« {len(logs)} è¡Œï¼Œä»…æ˜¾ç¤ºæœ€è¿‘ 500 è¡Œã€‚å®Œæ•´æ—¥å¿—ä¿å­˜åœ¨ `logs/{selected_task_id}.log`")
                    else:
                        st.info("æš‚æ— æ—¥å¿—è¾“å‡º")
                    
                    # æ·»åŠ è‡ªåŠ¨åˆ·æ–°æç¤º
                    st.caption("ğŸ’¡ æç¤ºï¼šæ—¥å¿—ä¼šå®æ—¶æ›´æ–°ï¼Œç‚¹å‡»ã€Œåˆ·æ–°æ—¥å¿—ã€æŒ‰é’®æˆ–åˆ·æ–°é¡µé¢æŸ¥çœ‹æœ€æ–°æ—¥å¿—")
    
    # ä»»åŠ¡ç®¡ç†é¡µé¢
    elif page == "ä»»åŠ¡ç®¡ç†":
        st.header("ğŸ“‹ ä»»åŠ¡ç®¡ç†")
        
        if not st.session_state.tasks:
            st.info("æš‚æ— ä»»åŠ¡è®°å½•")
        else:
            # ä»»åŠ¡ç­›é€‰
            col1, col2 = st.columns(2)
            with col1:
                status_filter = st.selectbox(
                    "ç­›é€‰çŠ¶æ€",
                    options=["å…¨éƒ¨", "running", "completed", "failed", "stopped", "pending"]
                )
            with col2:
                search_keyword = st.text_input("æœç´¢ä»»åŠ¡IDæˆ–è®­ç»ƒç±»å‹")
            
            # ç­›é€‰ä»»åŠ¡
            filtered_tasks = st.session_state.tasks.copy()
            if status_filter != "å…¨éƒ¨":
                filtered_tasks = {k: v for k, v in filtered_tasks.items() if v['status'] == status_filter}
            if search_keyword:
                filtered_tasks = {
                    k: v for k, v in filtered_tasks.items()
                    if search_keyword.lower() in k.lower() or
                    search_keyword.lower() in v.get('config', {}).get('training_type', '').lower()
                }
            
            # ä»»åŠ¡åˆ—è¡¨
            for task_id, task in filtered_tasks.items():
                expander_expanded = (st.session_state.get('view_logs_task') == task_id)
                with st.expander(f"ä»»åŠ¡: {task_id} | çŠ¶æ€: {task['status']} | ç±»å‹: {task.get('config', {}).get('training_type', 'N/A')}", expanded=expander_expanded):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.json(task.get('config', {}))
                        
                        if 'start_time' in task:
                            st.write(f"å¼€å§‹æ—¶é—´: {task['start_time']}")
                        if 'end_time' in task:
                            st.write(f"ç»“æŸæ—¶é—´: {task['end_time']}")
                        if 'error' in task:
                            st.error(f"é”™è¯¯ä¿¡æ¯: {task['error']}")
                    
                    with col2:
                        if task['status'] == 'running':
                            if st.button("åœæ­¢ä»»åŠ¡", key=f"stop_{task_id}"):
                                if stop_training_task(task_id):
                                    st.success("ä»»åŠ¡å·²åœæ­¢")
                                    st.rerun()
                        
                        if st.button("æŸ¥çœ‹æ—¥å¿—", key=f"logs_{task_id}"):
                            st.session_state['view_logs_task'] = task_id
                            st.rerun()
                        
                        if st.button("åˆ é™¤ä»»åŠ¡", key=f"delete_{task_id}"):
                            if task['status'] == 'running':
                                st.warning("è¯·å…ˆåœæ­¢è¿è¡Œä¸­çš„ä»»åŠ¡")
                            else:
                                del st.session_state.tasks[task_id]
                                if task_id in st.session_state.task_processes:
                                    del st.session_state.task_processes[task_id]
                                if task_id in st.session_state.task_logs:
                                    del st.session_state.task_logs[task_id]
                                if task_id in st.session_state.task_metrics:
                                    del st.session_state.task_metrics[task_id]
                                save_tasks(st.session_state.tasks)
                                st.success("ä»»åŠ¡å·²åˆ é™¤")
                                st.rerun()
                    
                    # æ˜¾ç¤ºæ—¥å¿—åŒºåŸŸï¼ˆåŒ…å«å¯è§†åŒ–å›¾è¡¨ï¼‰
                    if st.session_state.get('view_logs_task') == task_id:
                        st.markdown("---")
                        
                        # ä»»åŠ¡ä¿¡æ¯å¡ç‰‡
                        col_info1, col_info2, col_info3, col_info4 = st.columns(4)
                        with col_info1:
                            st.metric("ä»»åŠ¡çŠ¶æ€", task['status'])
                        with col_info2:
                            if 'start_time' in task:
                                start_time = datetime.fromisoformat(task['start_time'])
                                elapsed = datetime.now() - start_time
                                st.metric("è¿è¡Œæ—¶é—´", f"{elapsed.seconds // 60}åˆ†é’Ÿ")
                        with col_info3:
                            if 'pid' in task:
                                st.metric("è¿›ç¨‹ID", task['pid'])
                        with col_info4:
                            if 'config' in task:
                                st.metric("è®­ç»ƒç±»å‹", task['config'].get('training_type', 'N/A'))
                        
                        # è®­ç»ƒæŒ‡æ ‡å›¾è¡¨
                        # ä¼˜å…ˆä»æ—¥å¿—æ–‡ä»¶æå–æŒ‡æ ‡ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…å­˜ä¸­çš„
                        file_logs = load_task_logs_from_file(task_id)
                        if file_logs:
                            metrics = extract_metrics_from_logs(file_logs)
                        elif task_id in st.session_state.task_metrics:
                            metrics = st.session_state.task_metrics[task_id]
                        else:
                            metrics = {'loss': [], 'lr': [], 'step': [], 'epoch': [], 'timestamp': []}
                        
                        plot_training_metrics(metrics)
                        
                        # æ—¥å¿—æ–‡æœ¬æ˜¾ç¤ºåŒºåŸŸ
                        st.markdown("---")
                        st.subheader(f"ğŸ“‹ ä»»åŠ¡æ—¥å¿—: {task_id}")
                        
                        # ä»æ–‡ä»¶è¯»å–æ—¥å¿—
                        if file_logs:
                            logs = file_logs
                        elif task_id in st.session_state.task_logs:
                            logs = st.session_state.task_logs[task_id]
                        else:
                            logs = []
                        
                        if logs:
                            # æ˜¾ç¤ºæœ€è¿‘500è¡Œæ—¥å¿—
                            display_logs = logs[-500:] if len(logs) > 500 else logs
                            log_text = "\n".join(display_logs)
                            st.code(log_text, language=None)
                            st.caption(f"æ˜¾ç¤ºæœ€è¿‘ {len(display_logs)} è¡Œæ—¥å¿—ï¼ˆå…± {len(logs)} è¡Œï¼‰")
                            if len(logs) > 500:
                                st.info(f"ğŸ’¡ å®Œæ•´æ—¥å¿—ä¿å­˜åœ¨ `logs/{task_id}.log`")
                        else:
                            st.info("æš‚æ— æ—¥å¿—è¾“å‡ºã€‚æ—¥å¿—æ–‡ä»¶å¯èƒ½å°šæœªåˆ›å»ºæˆ–ä»»åŠ¡åˆšåˆšå¯åŠ¨ã€‚")
                        
                        if st.button("å…³é—­æ—¥å¿—", key=f"close_logs_{task_id}", use_container_width=True):
                            st.session_state['view_logs_task'] = None
                            st.rerun()
    
    # æ¨¡å‹ç®¡ç†é¡µé¢
    elif page == "æ¨¡å‹ç®¡ç†":
        st.header("ğŸ“¦ æ¨¡å‹ç®¡ç†")
        
        # åˆ›å»ºä¸¤ä¸ªæ ‡ç­¾é¡µï¼šPyTorchæ¨¡å‹å’ŒTransformersæ¨¡å‹
        tab1, tab2 = st.tabs(["PyTorchæ ¼å¼æ¨¡å‹ (.pth)", "Transformersæ ¼å¼æ¨¡å‹"])
        
        # ========== PyTorchæ ¼å¼æ¨¡å‹ ==========
        with tab1:
            out_dir = Path("../out")
            if out_dir.exists():
                model_files = list(out_dir.glob("*.pth"))
                
                if not model_files:
                    st.info("æœªæ‰¾åˆ°PyTorchæ ¼å¼çš„æ¨¡å‹æ–‡ä»¶")
                else:
                    st.write(f"æ‰¾åˆ° {len(model_files)} ä¸ªPyTorchæ¨¡å‹æ–‡ä»¶")
                    
                    for model_file in model_files:
                        with st.expander(f"æ¨¡å‹: {model_file.name}"):
                            col1, col2, col3 = st.columns(3)
                            
                            with col1:
                                file_size = model_file.stat().st_size / (1024 * 1024)  # MB
                                st.metric("æ–‡ä»¶å¤§å°", f"{file_size:.2f} MB")
                                st.write(f"è·¯å¾„: {model_file}")
                            
                            with col2:
                                mtime = datetime.fromtimestamp(model_file.stat().st_mtime)
                                st.metric("ä¿®æ”¹æ—¶é—´", mtime.strftime("%Y-%m-%d %H:%M:%S"))
                            
                            with col3:
                                if st.button("ä¸‹è½½", key=f"download_{model_file.name}"):
                                    with open(model_file, 'rb') as f:
                                        st.download_button(
                                            "ç‚¹å‡»ä¸‹è½½",
                                            f.read(),
                                            file_name=model_file.name,
                                            mime="application/octet-stream",
                                            key=f"dl_{model_file.name}"
                                        )
                                
                                # è½¬æ¢æ ¼å¼æŒ‰é’®
                                if st.button("ğŸ”„ è½¬æ¢ä¸ºTransformersæ ¼å¼", key=f"convert_{model_file.name}"):
                                    st.session_state[f'show_convert_{model_file.name}'] = True
                            
                            # è½¬æ¢æ ¼å¼å¯¹è¯æ¡†
                            if st.session_state.get(f'show_convert_{model_file.name}', False):
                                with st.expander("æ¨¡å‹æ ¼å¼è½¬æ¢ (PyTorch â†’ Transformers)", expanded=True):
                                    st.write(f"**æºæ–‡ä»¶**: {model_file.name}")
                                    
                                    # è§£ææ¨¡å‹é…ç½®
                                    try:
                                        model_config = parse_model_config_from_filename(str(model_file))
                                        st.info(f"æ£€æµ‹åˆ°çš„é…ç½®: hidden_size={model_config['hidden_size']}, "
                                              f"num_hidden_layers={model_config['num_hidden_layers']}, "
                                              f"use_moe={model_config['use_moe']}")
                                    except Exception as e:
                                        st.error(f"è§£ææ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
                                        st.session_state[f'show_convert_{model_file.name}'] = False
                                        st.stop()
                                    
                                    # è½¬æ¢ç±»å‹é€‰æ‹©
                                    convert_type = st.radio(
                                        "è½¬æ¢æ ¼å¼",
                                        options=["llama", "minimind"],
                                        format_func=lambda x: "Llamaå…¼å®¹æ ¼å¼ï¼ˆæ¨èï¼‰" if x == "llama" else "MiniMindåŸç”Ÿæ ¼å¼",
                                        index=0,
                                        help="Llamaæ ¼å¼å…¼å®¹æ›´å¤šç¬¬ä¸‰æ–¹å·¥å…·ï¼ˆvllmã€ollamaç­‰ï¼‰"
                                    )
                                    
                                    # æ•°æ®ç±»å‹é€‰æ‹©
                                    dtype = st.selectbox(
                                        "æ•°æ®ç±»å‹",
                                        options=["float16", "bfloat16"],
                                        index=0,
                                        help="float16å…¼å®¹æ€§æ›´å¥½ï¼Œbfloat16ç²¾åº¦æ›´é«˜"
                                    )
                                    
                                    # è¾“å‡ºè·¯å¾„
                                    default_output_name = f"{model_file.stem}_transformers"
                                    output_name = st.text_input(
                                        "è¾“å‡ºç›®å½•å",
                                        value=default_output_name,
                                        help="å°†åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»ºæ­¤ç›®å½•"
                                    )
                                    output_path = str(Path(f"../{output_name}").resolve())
                                    
                                    col_conv1, col_conv2 = st.columns(2)
                                    
                                    with col_conv1:
                                        if st.button("âœ… å¼€å§‹è½¬æ¢", key=f"do_convert_{model_file.name}", use_container_width=True):
                                            if not output_name.strip():
                                                st.error("è¯·è¾“å…¥è¾“å‡ºç›®å½•å")
                                            else:
                                                # æ£€æŸ¥è¾“å‡ºç›®å½•æ˜¯å¦å·²å­˜åœ¨
                                                if Path(output_path).exists():
                                                    st.warning(f"è¾“å‡ºç›®å½• {output_path} å·²å­˜åœ¨ï¼Œè½¬æ¢å°†è¦†ç›–ç°æœ‰æ–‡ä»¶")
                                                
                                                with st.spinner("æ­£åœ¨è½¬æ¢æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                                                    success, message = convert_torch_to_transformers(
                                                        str(model_file.resolve()),
                                                        output_path,
                                                        model_config,
                                                        convert_type=convert_type,
                                                        dtype=dtype
                                                    )
                                                
                                                if success:
                                                    st.success(message)
                                                    st.balloons()
                                                    # æ¸…ç†çŠ¶æ€
                                                    st.session_state[f'show_convert_{model_file.name}'] = False
                                                    st.rerun()
                                                else:
                                                    st.error(message)
                                    
                                    with col_conv2:
                                        if st.button("âŒ å–æ¶ˆ", key=f"cancel_convert_{model_file.name}", use_container_width=True):
                                            st.session_state[f'show_convert_{model_file.name}'] = False
                                            st.rerun()
            else:
                st.warning("è¾“å‡ºç›®å½•ä¸å­˜åœ¨: ../out")
        
        # ========== Transformersæ ¼å¼æ¨¡å‹ ==========
        with tab2:
            # æ‰«æTransformersæ ¼å¼æ¨¡å‹ç›®å½•
            model_dirs = [
                Path("../MiniMind2"),
                Path("../MiniMind2-Small"),
                Path("../MiniMind2-MoE"),
                Path("../MiniMind2-R1"),
                Path("../MiniMind2-Small-R1"),
            ]
            
            # æ‰«æé¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰€æœ‰åŒ…å«config.jsonçš„ç›®å½•
            root_dir = Path("..")
            transformers_models = []
            for model_dir in model_dirs:
                if model_dir.exists() and (model_dir / "config.json").exists():
                    transformers_models.append(model_dir)
            
            # æ‰«æå…¶ä»–å¯èƒ½çš„æ¨¡å‹ç›®å½•ï¼ˆåç§°åŒ…å«transformersçš„ï¼‰
            for item in root_dir.iterdir():
                if item.is_dir() and (item / "config.json").exists():
                    if item not in transformers_models and not item.name.startswith('.'):
                        transformers_models.append(item)
            
            if not transformers_models:
                st.info("æœªæ‰¾åˆ°Transformersæ ¼å¼çš„æ¨¡å‹ç›®å½•")
            else:
                st.write(f"æ‰¾åˆ° {len(transformers_models)} ä¸ªTransformersæ¨¡å‹ç›®å½•")
                
                for model_dir in transformers_models:
                    with st.expander(f"æ¨¡å‹: {model_dir.name}"):
                        col1, col2, col3 = st.columns(3)
                        
                        with col1:
                            # è®¡ç®—ç›®å½•å¤§å°
                            total_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
                            dir_size_mb = total_size / (1024 * 1024)
                            st.metric("ç›®å½•å¤§å°", f"{dir_size_mb:.2f} MB")
                            st.write(f"è·¯å¾„: {model_dir}")
                        
                        with col2:
                            # è·å–config.jsonçš„ä¿®æ”¹æ—¶é—´
                            config_file = model_dir / "config.json"
                            if config_file.exists():
                                mtime = datetime.fromtimestamp(config_file.stat().st_mtime)
                                st.metric("ä¿®æ”¹æ—¶é—´", mtime.strftime("%Y-%m-%d %H:%M:%S"))
                            else:
                                st.metric("ä¿®æ”¹æ—¶é—´", "æœªçŸ¥")
                        
                        with col3:
                            # è½¬æ¢æ ¼å¼æŒ‰é’®
                            model_key = f"convert_tf_{model_dir.name}"
                            if st.button("ğŸ”„ è½¬æ¢ä¸ºPyTorchæ ¼å¼", key=model_key):
                                st.session_state[f'show_convert_tf_{model_dir.name}'] = True
                        
                        # è½¬æ¢æ ¼å¼å¯¹è¯æ¡†
                        if st.session_state.get(f'show_convert_tf_{model_dir.name}', False):
                            with st.expander("æ¨¡å‹æ ¼å¼è½¬æ¢ (Transformers â†’ PyTorch)", expanded=True):
                                st.write(f"**æºç›®å½•**: {model_dir.name}")
                                
                                # è¾“å‡ºæ–‡ä»¶å
                                default_output_name = f"{model_dir.name}.pth"
                                output_name = st.text_input(
                                    "è¾“å‡ºæ–‡ä»¶å",
                                    value=default_output_name,
                                    help="å°†åœ¨ ../out/ ç›®å½•ä¸‹åˆ›å»ºæ­¤æ–‡ä»¶",
                                    key=f"output_name_tf_{model_dir.name}"
                                )
                                output_path = str((Path("../out") / output_name).resolve())
                                
                                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                                Path("../out").mkdir(parents=True, exist_ok=True)
                                
                                col_conv1, col_conv2 = st.columns(2)
                                
                                with col_conv1:
                                    if st.button("âœ… å¼€å§‹è½¬æ¢", key=f"do_convert_tf_{model_dir.name}", use_container_width=True):
                                        if not output_name.strip():
                                            st.error("è¯·è¾“å…¥è¾“å‡ºæ–‡ä»¶å")
                                        else:
                                            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                                            if Path(output_path).exists():
                                                st.warning(f"è¾“å‡ºæ–‡ä»¶ {output_path} å·²å­˜åœ¨ï¼Œè½¬æ¢å°†è¦†ç›–ç°æœ‰æ–‡ä»¶")
                                            
                                            with st.spinner("æ­£åœ¨è½¬æ¢æ¨¡å‹ï¼Œè¯·ç¨å€™..."):
                                                success, message = convert_transformers_to_torch(
                                                    str(model_dir.resolve()),
                                                    output_path
                                                )
                                            
                                            if success:
                                                st.success(message)
                                                st.balloons()
                                                # æ¸…ç†çŠ¶æ€
                                                st.session_state[f'show_convert_tf_{model_dir.name}'] = False
                                                st.rerun()
                                            else:
                                                st.error(message)
                                
                                with col_conv2:
                                    if st.button("âŒ å–æ¶ˆ", key=f"cancel_convert_tf_{model_dir.name}", use_container_width=True):
                                        st.session_state[f'show_convert_tf_{model_dir.name}'] = False
                                        st.rerun()
    
    # æ¨¡å‹ä½¿ç”¨é¡µé¢
    elif page == "æ¨¡å‹ä½¿ç”¨":
        st.header("ğŸ’¬ æ¨¡å‹ä½¿ç”¨")
        
        # åˆå§‹åŒ–å¯¹è¯å†å²å’Œæ¨¡å‹çŠ¶æ€
        if 'chat_messages' not in st.session_state:
            st.session_state.chat_messages = []
        if 'current_model_path' not in st.session_state:
            st.session_state.current_model_path = None
        if 'current_model' not in st.session_state:
            st.session_state.current_model = None
        if 'current_tokenizer' not in st.session_state:
            st.session_state.current_tokenizer = None
        if 'model_last_used_time' not in st.session_state:
            st.session_state.model_last_used_time = None
        if 'auto_release_gpu' not in st.session_state:
            st.session_state.auto_release_gpu = True
        if 'gpu_release_timeout' not in st.session_state:
            st.session_state.gpu_release_timeout = 300  # é»˜è®¤5åˆ†é’Ÿæ— æ“ä½œè‡ªåŠ¨é‡Šæ”¾
        
        # è‡ªåŠ¨é‡Šæ”¾GPUæ£€æŸ¥ï¼ˆå¦‚æœè¶…è¿‡æŒ‡å®šæ—¶é—´æœªä½¿ç”¨ï¼‰
        if st.session_state.current_model is not None and st.session_state.auto_release_gpu:
            if st.session_state.model_last_used_time is not None:
                time_since_last_use = time.time() - st.session_state.model_last_used_time
                if time_since_last_use > st.session_state.gpu_release_timeout:
                    with st.spinner("æ£€æµ‹åˆ°æ¨¡å‹é•¿æ—¶é—´æœªä½¿ç”¨ï¼Œæ­£åœ¨è‡ªåŠ¨é‡Šæ”¾GPU..."):
                        if release_model_from_gpu():
                            st.info("âœ… GPUå·²è‡ªåŠ¨é‡Šæ”¾ï¼ˆæ¨¡å‹è¶…è¿‡5åˆ†é’Ÿæœªä½¿ç”¨ï¼‰")
                            st.rerun()
        
        # è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼ˆåœ¨ä¸»åŒºåŸŸä¹Ÿä½¿ç”¨ï¼‰
        available_models = get_available_models()
        
        # åˆå§‹åŒ–è®¾å¤‡é€‰æ‹©
        if 'inference_device' not in st.session_state:
            st.session_state.inference_device = "cuda:0" if torch.cuda.is_available() else "cpu"
        
        # ä¾§è¾¹æ é…ç½®
        with st.sidebar:
            st.subheader("æ¨¡å‹é…ç½®")
            
            if not available_models:
                st.warning("æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹\n\nè¯·ç¡®ä¿ï¼š\n1. Transformersæ ¼å¼æ¨¡å‹åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼ˆåŒ…å«config.jsonæ–‡ä»¶ï¼‰\n2. å¦‚éœ€ä½¿ç”¨PyTorchæ¨¡å‹ï¼Œè¯·å…ˆåœ¨ã€Œæ¨¡å‹ç®¡ç†ã€é¡µé¢è½¬æ¢ä¸ºTransformersæ ¼å¼")
            else:
                # æ¨¡å‹é€‰æ‹©ï¼ˆåªæ˜¾ç¤ºTransformersæ ¼å¼æ¨¡å‹ï¼‰
                model_options = {info['name']: path 
                                for path, info in available_models.items()}
                selected_model_display = st.selectbox(
                    "é€‰æ‹©æ¨¡å‹ (Transformersæ ¼å¼)",
                    options=list(model_options.keys()),
                    index=0,
                    help="ä»…æ˜¾ç¤ºTransformersæ ¼å¼æ¨¡å‹ï¼Œå¦‚éœ€ä½¿ç”¨PyTorchæ¨¡å‹è¯·å…ˆåœ¨ã€Œæ¨¡å‹ç®¡ç†ã€é¡µé¢è½¬æ¢"
                )
                selected_model_path = model_options[selected_model_display]
                selected_model_info = available_models[selected_model_path]
                
                # è®¾å¤‡é€‰æ‹©
                device = st.selectbox(
                    "è¿è¡Œè®¾å¤‡",
                    options=["cuda:0", "cuda:1", "cpu"],
                    index=0 if torch.cuda.is_available() else 2,
                    key="device_selector"
                )
                st.session_state.inference_device = device
                
                # GPUè‡ªåŠ¨é‡Šæ”¾è®¾ç½®
                st.markdown("---")
                st.subheader("GPUç®¡ç†")
                auto_release = st.checkbox(
                    "è‡ªåŠ¨é‡Šæ”¾GPU",
                    value=st.session_state.auto_release_gpu,
                    help="åˆ‡æ¢é¡µé¢æˆ–è¶…è¿‡æŒ‡å®šæ—¶é—´æœªä½¿ç”¨æ—¶è‡ªåŠ¨é‡Šæ”¾GPU"
                )
                st.session_state.auto_release_gpu = auto_release
                
                if auto_release:
                    timeout_minutes = st.number_input(
                        "è‡ªåŠ¨é‡Šæ”¾æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰",
                        min_value=1,
                        max_value=60,
                        value=int(st.session_state.gpu_release_timeout / 60),
                        help="è¶…è¿‡æ­¤æ—¶é—´æœªä½¿ç”¨æ¨¡å‹å°†è‡ªåŠ¨é‡Šæ”¾GPU"
                    )
                    st.session_state.gpu_release_timeout = timeout_minutes * 60
                
                # åŠ è½½æ¨¡å‹æŒ‰é’®
                if st.button("ğŸ”„ åŠ è½½æ¨¡å‹", use_container_width=True):
                    # å¦‚æœå·²æœ‰æ¨¡å‹ï¼Œå…ˆé‡Šæ”¾
                    if st.session_state.current_model is not None:
                        release_model_from_gpu()
                    
                    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹..."):
                        model, tokenizer = load_model_for_inference(
                            selected_model_path, 
                            selected_model_info,
                            device=st.session_state.inference_device
                        )
                        if model is not None:
                            st.session_state.current_model = model
                            st.session_state.current_tokenizer = tokenizer
                            st.session_state.current_model_path = selected_model_path
                            st.session_state.model_last_used_time = time.time()
                            st.success("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
                            # åˆ‡æ¢æ¨¡å‹æ—¶æ¸…ç©ºå¯¹è¯å†å²
                            st.session_state.chat_messages = []
                        else:
                            st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
                
                # æ‰‹åŠ¨é‡Šæ”¾GPUæŒ‰é’®
                if st.session_state.current_model is not None:
                    if st.button("ğŸ—‘ï¸ é‡Šæ”¾GPU", use_container_width=True):
                        if release_model_from_gpu():
                            st.success("âœ… GPUå·²é‡Šæ”¾")
                            st.rerun()
                
                st.markdown("---")
                st.subheader("ç”Ÿæˆå‚æ•°")
                
                temperature = st.slider(
                    "Temperature (æ¸©åº¦)",
                    min_value=0.1,
                    max_value=2.0,
                    value=0.85,
                    step=0.05,
                    help="æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œå€¼è¶Šå¤§è¶Šéšæœº"
                )
                
                max_new_tokens = st.slider(
                    "Max New Tokens (æœ€å¤§ç”Ÿæˆé•¿åº¦)",
                    min_value=128,
                    max_value=8192,
                    value=2048,
                    step=128,
                    help="æ¨¡å‹ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡"
                )
                
                top_p = st.slider(
                    "Top-p (æ ¸é‡‡æ ·)",
                    min_value=0.1,
                    max_value=1.0,
                    value=0.85,
                    step=0.05,
                    help="nucleusé‡‡æ ·é˜ˆå€¼"
                )
                
                history_chat_num = st.slider(
                    "å†å²å¯¹è¯è½®æ•°",
                    min_value=0,
                    max_value=10,
                    value=0,
                    step=2,
                    help="ä¿ç•™çš„å†å²å¯¹è¯è½®æ•°ï¼ˆ0è¡¨ç¤ºä¸ä½¿ç”¨å†å²ï¼‰"
                )
                
                if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", use_container_width=True):
                    st.session_state.chat_messages = []
                    st.rerun()
        
        # ä¸»å¯¹è¯åŒºåŸŸ
        if st.session_state.current_model is None:
            st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ é€‰æ‹©æ¨¡å‹å¹¶ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€æŒ‰é’®")
        else:
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯å’ŒGPUçŠ¶æ€
            col1, col2, col3 = st.columns([2, 1, 1])
            with col1:
                # ä»session_stateè·å–æ¨¡å‹è·¯å¾„å¯¹åº”çš„ä¿¡æ¯
                current_path = st.session_state.current_model_path
                if current_path and current_path in available_models:
                    model_name = available_models[current_path]['name']
                else:
                    model_name = "æœªçŸ¥æ¨¡å‹"
                st.success(f"âœ… å½“å‰ä½¿ç”¨æ¨¡å‹: {model_name}")
            
            with col2:
                if st.session_state.model_last_used_time:
                    last_use_ago = int(time.time() - st.session_state.model_last_used_time)
                    st.caption(f"æœ€åä½¿ç”¨: {last_use_ago // 60}åˆ†{last_use_ago % 60}ç§’å‰")
            
            with col3:
                if torch.cuda.is_available() and st.session_state.inference_device.startswith("cuda"):
                    try:
                        device_id = int(st.session_state.inference_device.split(":")[1])
                        gpu_memory = torch.cuda.get_device_properties(device_id).total_memory / 1e9
                        gpu_allocated = torch.cuda.memory_allocated(device_id) / 1e9
                        st.caption(f"GPUæ˜¾å­˜: {gpu_allocated:.1f}GB / {gpu_memory:.1f}GB")
                    except:
                        st.caption("GPUä¿¡æ¯è·å–å¤±è´¥")
            
            # æ˜¾ç¤ºå¯¹è¯å†å²
            for i, msg in enumerate(st.session_state.chat_messages):
                if msg["role"] == "user":
                    st.markdown(
                        f'<div style="display: flex; justify-content: flex-end; margin: 10px 0;">'
                        f'<div style="display: inline-block; padding: 10px 15px; background-color: #007bff; '
                        f'border-radius: 15px; color: white; max-width: 70%;">'
                        f'{msg["content"]}'
                        f'</div></div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        f'<div style="display: flex; justify-content: flex-start; margin: 10px 0;">'
                        f'<div style="display: inline-block; padding: 10px 15px; background-color: #f0f0f0; '
                        f'border-radius: 15px; max-width: 70%;">'
                        f'{process_assistant_content(msg["content"])}'
                        f'</div></div>',
                        unsafe_allow_html=True
                    )
            
            # è¾“å…¥æ¡†
            user_input = st.chat_input("è¾“å…¥æ¶ˆæ¯...")
            
            if user_input:
                # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                st.session_state.model_last_used_time = time.time()
                
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                st.session_state.chat_messages.append({"role": "user", "content": user_input})
                
                # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
                st.markdown(
                    f'<div style="display: flex; justify-content: flex-end; margin: 10px 0;">'
                    f'<div style="display: inline-block; padding: 10px 15px; background-color: #007bff; '
                    f'border-radius: 15px; color: white; max-width: 70%;">'
                    f'{user_input}'
                    f'</div></div>',
                    unsafe_allow_html=True
                )
                
                # ç”Ÿæˆå›å¤
                with st.spinner("æ€è€ƒä¸­..."):
                    try:
                        model = st.session_state.current_model
                        tokenizer = st.session_state.current_tokenizer
                        
                        # å‡†å¤‡å¯¹è¯å†å²
                        history_messages = st.session_state.chat_messages
                        if history_chat_num > 0:
                            history_messages = history_messages[-(history_chat_num + 1):]
                        
                        # åº”ç”¨èŠå¤©æ¨¡æ¿
                        try:
                            prompt = tokenizer.apply_chat_template(
                                history_messages,
                                tokenize=False,
                                add_generation_prompt=True
                            )
                        except:
                            # å¦‚æœæ²¡æœ‰chat_templateï¼Œä½¿ç”¨ç®€å•æ ¼å¼
                            prompt = "\n".join([
                                f"{'ç”¨æˆ·' if m['role'] == 'user' else 'åŠ©æ‰‹'}: {m['content']}"
                                for m in history_messages
                            ]) + "\nåŠ©æ‰‹: "
                        
                        # Tokenize
                        inputs = tokenizer(
                            prompt,
                            return_tensors="pt",
                            truncation=True
                        ).to(st.session_state.inference_device)
                        
                        # æµå¼ç”Ÿæˆ
                        from transformers import TextIteratorStreamer
                        from threading import Thread
                        
                        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
                        
                        generation_kwargs = {
                            "input_ids": inputs.input_ids,
                            "attention_mask": inputs.attention_mask,
                            "max_new_tokens": max_new_tokens,
                            "do_sample": True,
                            "temperature": temperature,
                            "top_p": top_p,
                            "pad_token_id": tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,
                            "eos_token_id": tokenizer.eos_token_id,
                            "streamer": streamer,
                        }
                        
                        # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
                        placeholder = st.empty()
                        thread = Thread(target=model.generate, kwargs=generation_kwargs)
                        thread.start()
                        
                        # æµå¼æ˜¾ç¤º
                        answer = ""
                        for text in streamer:
                            answer += text
                            placeholder.markdown(
                                f'<div style="display: flex; justify-content: flex-start; margin: 10px 0;">'
                                f'<div style="display: inline-block; padding: 10px 15px; background-color: #f0f0f0; '
                                f'border-radius: 15px; max-width: 70%;">'
                                f'{process_assistant_content(answer)}'
                                f'</div></div>',
                                unsafe_allow_html=True
                            )
                        
                        # æ·»åŠ åˆ°å¯¹è¯å†å²
                        st.session_state.chat_messages.append({"role": "assistant", "content": answer})
                        
                        # æ›´æ–°æœ€åä½¿ç”¨æ—¶é—´
                        st.session_state.model_last_used_time = time.time()
                        
                    except Exception as e:
                        st.error(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
                        import traceback
                        st.code(traceback.format_exc())


if __name__ == "__main__":
    main()

